{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to read raw reddit json files and convert them to parquet\n",
    "- working with parquet files is much faster than json ones\n",
    "\n",
    "## First imports\n",
    "- we neet to get an `sqlContext` from spark to do our work in\n",
    "- The `sc` (spark context) variable is already set for us, we can just use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sqlC = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in json data\n",
    "- all the comments data is in /var/reddit\n",
    "    - the comments data is in files by month and begins with `RC`\n",
    "    - the submissions data (things comments reply to, \"first posts\" in each thread) is in files by month and begins with `RS`\n",
    "- For all comments, this takes about half an hour on UM's flux hadoop cluster.\n",
    "    - you can make it faster if you use just the files for a specific year or month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of reddit:\n",
    "#reddit = sqlC.read.json(\"/var/reddit/RC_*\")\n",
    "\n",
    "#one month\n",
    "reddit_raw = sqlC.read.json(\"/var/reddit/RC_2010-02\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to a faster format\n",
    "- spark works much faster on .parquet formatted files. What takes minutes or hours in json is seconds in parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_raw.write.parquet('2010_02_comments.parquet', mode='overwrite')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = sqlC.read.parquet('2010_02_comments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed difference\n",
    "- Difference grows exponentially, really appears in larger data or more complicated operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit reddit_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit reddit.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe style\n",
    "reddit[['id', 'author', 'subreddit', 'body']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql style\n",
    "reddit.select('id', 'author', 'subreddit', 'body').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe style\n",
    "reddit[reddit.subreddit == 'politics'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql style\n",
    "reddit.filter(reddit.subreddit == 'politics').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[reddit.body.contains('kitten') | reddit.body.contains('Kitten')].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### built-in pyspark functions\n",
    "e.g. lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[lower(reddit.body).contains('kitten')].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing data\n",
    "- We can't modify or assign things to pyspark dataframes like normal\n",
    "- There are special functions for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit.withColumnRenamed('id', 'comment_id')\n",
    "reddit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit.withColumn('date', from_unixtime('created_utc'))\n",
    "reddit[['comment_id', 'subreddit', 'created_utc', 'date']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit.withColumn('body', regexp_replace('body', '\\t', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More fun examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.groupBy('author').count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit[reddit.author != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.groupBy('subreddit').count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.groupBy('subreddit', 'author').count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = reddit.groupBy('subreddit', 'author').count()\n",
    "tmp = tmp.groupby('subreddit').mean()\n",
    "tmp.sort(desc('avg(count)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.withColumnRenamed('avg(count)', 'comments_per_user').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.select('subreddit', col('avg(count)').alias('comments_per_user2')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "### for networks\n",
    "- making a network of authors who participate in the same subreddits by using a self-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = reddit[['subreddit', 'author']]\n",
    "author_network = tmp.join(tmp[['subreddit', col('author').alias('author2')]], on='subreddit')\n",
    "author_network = author_network.drop_duplicates()\n",
    "author_network.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for sets\n",
    "- Sometimes you want everyone who *isn't* in one group, and we have ant-join for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_users = reddit[reddit.subreddit == 'funny'][['author']].distinct()\n",
    "funny_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_users = reddit[reddit.subreddit == 'science'][['author']].distinct()\n",
    "science_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users in both subreddits\n",
    "science_users.join(funny_users, on='author', how='inner').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users in only science\n",
    "science_users.join(funny_users, on='author', how='left_anti').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_scientists = reddit[reddit['subreddit'] == 'science']\n",
    "funny_scientists = funny_scientists.join(funny_users, on='author', how='inner')\n",
    "funny_scientists[['body']].show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boring_scientists = reddit[reddit['subreddit'] == 'science']\n",
    "boring_scientists = boring_scientists.join(funny_users, on='author', how='left_anti')\n",
    "boring_scientists[['body']].show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_scientists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_df = funny_scientists.toPandas()\n",
    "fs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_df.to_csv('funny_scientists.tsv', sep='\\t', index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
